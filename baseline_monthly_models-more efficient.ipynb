{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import aggregator as ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_file_name_review = \"distributed_data_collection/databases/review_data_sample.csv\"\n",
    "#data_file_name_book = \"distributed_data_collection/databases/book_data_sample.csv\"\n",
    "\n",
    "data_file_name_review = \"distributed_data_collection/databases/review_data.csv\"\n",
    "data_file_name_book = \"distributed_data_collection/databases/book_data.csv\"\n",
    "\n",
    "start_date = datetime.datetime(2018, 1, 1)\n",
    "end_date = datetime.datetime(2020, 11, 29)\n",
    "\n",
    "#book_columns = [\"num_reviews\", \"num_ratings\", \"avg_rating\"] ##THIS IS BASELINE\n",
    "book_columns = [\"num_reviews\", \"num_ratings\", \"avg_rating\", \"book_language\", \"series\", \"book_author\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, we could test against multiple grains (ie, day, week, month, quarter). However, I am only working with month. I ruled out the quarterly grain because it wouldn't include enough 2020 time periods to observe a trend in model performance over time. Then because the maximum monthly review counts were in the seventies, I didn't feel I could split those further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator Initiated.\n",
      "Processing Scraper Output...\n"
     ]
    }
   ],
   "source": [
    "data_aggregator = ag.Aggregator(data_file_name_review, data_file_name_book, book_columns, start_date, end_date, \"month\")\n",
    "data = data_aggregator.aggregate(\"by_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to drop binary feature columns which have a small number of values. The linear regression regularization will do a rigorous feature selection, so this step is mostly to reduce the amount of data that it has to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dropped = []\n",
    "k = 2\n",
    "\n",
    "for col in data.columns:\n",
    "    \n",
    "    num_values = data[col].nunique()\n",
    "    \n",
    "    if num_values == 1:\n",
    "        columns_dropped.append(col)\n",
    "        \n",
    "    elif num_values == 2:\n",
    "        if data[col].sum() <=2:\n",
    "            columns_dropped.append(col)\n",
    "            \n",
    "for col in columns_dropped:\n",
    "    data.drop(columns = col, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get a sense of how many features got dropped. I assume a ton!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will take all the 2018-2019 periods as features and will use those to predict reviews for each 2020 period. Differences in the overall accuracy of each model as well as shifts in the importance of different features to the model may indicate COVID-related change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods_post = []\n",
    "\n",
    "for col in data.columns:\n",
    "    if \"review_count\" in col:\n",
    "        if \"2020\" in col:\n",
    "            time_periods_post.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use the same test/train split for every time period, so I need to make the split before selecting which period will be the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train = 0.75\n",
    "\n",
    "data = data.iloc[np.random.permutation(data.index)].reset_index(drop=True) #SHUFFLES DATA\n",
    "\n",
    "num_observations_total = len(data)\n",
    "num_observations_train = int(num_observations_total* perc_train)\n",
    "num_observations_test = num_observations_total - num_observations_train\n",
    "\n",
    "data_train = data.head(num_observations_train).reset_index(drop = True)\n",
    "data_test = data.tail(num_observations_test).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to iterate through: logorithmic transformation, regularization type, and alpha value. For the moment, I am turning logorithmic transformation off. It's pretty clear that it has no impact, and I don't want to keep running it while I experiment with other aspects of the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = list(range(1,11))\n",
    "regression_types_list = [\"linear\", \"ridge\", \"lasso\"]\n",
    "\n",
    "#is_log_options = [False, True]\n",
    "is_log_options = [False] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes on modeling! We will select the best model for each time period based on MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST FOR PROGRESS PRINTING\n",
    "\n",
    "num_models = len(regression_types_list)\n",
    "num_periods = len(time_periods_post)\n",
    "num_is_log_options = len(is_log_options)\n",
    "num_alphas = len(alpha_list)\n",
    "\n",
    "num_models_total = (num_periods * num_is_log_options) * ( (num_alphas * (num_models - 1)) + num_models)\n",
    "num_models_complete = 0\n",
    "model_dict = {}\n",
    "\n",
    "#ACTUAL MODELING\n",
    "performance_df = pd.DataFrame(columns = [\"post_period\", \"regression_type\", \"is_log\", \"alpha\", \"mse_train\", \"mse_test\", \"r2_train\", \"r2_test\"])\n",
    "\n",
    "for i in range(len(time_periods_post)): #ITERATE OVER EACH PERIOD IN THE POST PERIOD\n",
    "    \n",
    "    label = time_periods_post[i]\n",
    "    \n",
    "    data_train_period = data_train.copy()\n",
    "    data_test_period = data_test.copy()\n",
    "    \n",
    "    #REMOVE OTHER POST-PERIODS FROM DATA \n",
    "    \n",
    "    for post_period in time_periods_post:\n",
    "\n",
    "        if post_period != label:\n",
    "            data_train_period.drop(columns = post_period, inplace = True)\n",
    "            data_test_period.drop(columns = post_period, inplace = True)\n",
    "                   \n",
    "    #CREATE TRAINING & TESTING DATA SPECIFIC TO THAT PERIOD \n",
    "\n",
    "    x_train, y_train = data_train_period.drop(label,1), data_train_period[label]\n",
    "    x_test, y_test = data_test_period.drop(label,1), data_test_period[label]\n",
    "        \n",
    "    optimal_model = None\n",
    "    optimal_regression_type = None\n",
    "    optimal_mse_test = None\n",
    "    optimal_is_log = None\n",
    "    optimal_alpha = None\n",
    "    is_none = True\n",
    "    \n",
    "    for regression_type in regression_types_list:\n",
    "        for is_log in is_log_options:\n",
    "            for alpha_val in alpha_list:\n",
    "                \n",
    "                if regression_type == \"linear\":\n",
    "                    model = LinearRegression()\n",
    "                    \n",
    "                if regression_type == \"ridge\":\n",
    "                    model = Ridge(normalize = True, alpha = alpha_val) \n",
    "                    \n",
    "                if regression_type == \"lasso\":\n",
    "                    model = Lasso(normalize = True, alpha = alpha_val)\n",
    "            \n",
    "                model.fit(x_train, y_train)\n",
    "                mse_test = metrics.mean_squared_error(y_test, model.predict(x_test))\n",
    "\n",
    "                if is_none:\n",
    "                    optimal_model = model\n",
    "                    optimal_mse_test = mse_test\n",
    "                    optimal_regression_type = regression_type\n",
    "                    optimal_is_log = is_log\n",
    "                    \n",
    "                    if regression_type == \"linear\":\n",
    "                        alpha_val = None\n",
    "                    \n",
    "                    optimal_alpha = alpha_val\n",
    "\n",
    "                    is_none = False\n",
    "\n",
    "                elif mse_test < optimal_mse_test:\n",
    "\n",
    "                    optimal_model = model\n",
    "                    optimal_mse_test = mse_test\n",
    "                    optimal_regression_type = regression_type\n",
    "                    optimal_is_log = is_log\n",
    "                    \n",
    "                    if regression_type == \"linear\":\n",
    "                        alpha_val = None\n",
    "                    \n",
    "                    optimal_alpha = alpha_val\n",
    "\n",
    "                #PRINT UPDATES\n",
    "\n",
    "                num_models_complete +=1\n",
    "\n",
    "                if (num_models_complete % 10 == 0) or (num_models_complete == num_models_total):\n",
    "                    print(\"{}/{} models processed\".format(num_models_complete, num_models_total))\n",
    "                    \n",
    "                if regression_type == \"linear\":\n",
    "                    break\n",
    "                \n",
    "    #GET METRICS FOR WINNING MODEL\n",
    "    \n",
    "    mse_train = metrics.mean_squared_error(y_test, optimal_model.predict(x_test))\n",
    "    mse_test = metrics.mean_squared_error(y_test, optimal_model.predict(x_test))\n",
    "\n",
    "    r2_train = metrics.r2_score(y_train, optimal_model.predict(x_train))\n",
    "    r2_test = metrics.r2_score(y_test, optimal_model.predict(x_test))\n",
    "        \n",
    "    metric_dict = {\"post_period\": label, \"regression_type\": optimal_regression_type, \"is_log\": optimal_is_log, \"alpha\": optimal_alpha, \"mse_train\": mse_train, \"mse_test\": mse_test, \"r2_train\": r2_train, \"r2_test\": r2_test}\n",
    "    performance_df = performance_df.append(metric_dict, ignore_index=True)\n",
    "    model_dict[label] = optimal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df[\"post_period\"] = performance_df[\"post_period\"].apply(lambda text: text.replace(\"review_count \", \"\"))\n",
    "print(performance_df.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
