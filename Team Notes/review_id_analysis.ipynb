{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Because each data point costs more time than I would like, I would like to potentially reduce the range of URL IDs that we will have to scrape\n",
    "\n",
    "Background: Each review has a unique identifier in its URL, and our review scraper works by visiting the URLs associated with identifiers randomly selected from within a numerical range. I initially selected a range over the summer based on a preliminary scrape of about 6K URL IDs. My goal was to select a minimum ID cut-off that demark a range of reviews starting in January of 2018. However, because I wasn't confident in my analysis and wanted to be conservative, I selected a range of about 2.1 B reviews, which I estimated captured reviews starting in January 2017. \n",
    "\n",
    "At this point, I have significantly more data and more tool with which to analyze the problem more precisely. I would therefore like to revisit the analysis, in hopes of narrowing the range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data comes from the most up-to-date version of the review database. The columns should convey the following: \n",
    "\n",
    "id: the unique identifier which can be found in the URL of each review\n",
    "pub_date: the publication date of each review\n",
    "pub_date_ordinal: a transformation of the publication date\n",
    "pub_year: review publication year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'databases/review_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1ed18bf47680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Importing & Cleaning Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreview_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"databases/review_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is_URL_valid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"review_publication_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#review_df = review_df.head(500)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreview_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreview_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_URL_valid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#Removing IDS which do not have an associated review. In my previous analysis these appeared to be random. I hypothesize that they were once associated with reviews which have since been deleted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'databases/review_data.csv'"
     ]
    }
   ],
   "source": [
    "#Importing & Cleaning Data\n",
    "review_df = pd.read_csv(\"databases/review_data.csv\", usecols=[\"review_id\", \"is_URL_valid\", \"review_publication_date\"])\n",
    "#review_df = review_df.head(500)\n",
    "\n",
    "review_df = review_df[review_df.is_URL_valid == True] #Removing IDS which do not have an associated review. In my previous analysis these appeared to be random. I hypothesize that they were once associated with reviews which have since been deleted.\n",
    "review_df.drop(columns = \"is_URL_valid\", inplace = True)\n",
    "review_df.dropna(inplace = True)\n",
    "review_df.drop_duplicates(inplace = True)\n",
    "review_df.reset_index(inplace = True, drop = True)\n",
    "review_df.rename(columns = {\"review_id\": \"id\", \"review_publication_date\": \"pub_date\"}, inplace = True)\n",
    "\n",
    "#Transforming Publication Dates\n",
    "review_df[\"pub_date_ordinal\"] = pd.to_datetime(review_df[\"pub_date\"]).apply(lambda date: date.toordinal())\n",
    "review_df[\"pub_year\"] = pd.to_datetime(review_df[\"pub_date\"]).apply(lambda date: date.year)\n",
    " \n",
    "print(review_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am putting all my math and visualization stuff in this cell right here. I would love to collapse this code, but I don't think Jupyter does that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Visualizer():\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.df = df\n",
    "        \n",
    "        plt.close(\"all\") #BECAUSE JUPYTER NOTEBOOKS SUCK\n",
    "\n",
    "        with sns.axes_style(\"white\"):\n",
    "            ax = sns.regplot(x = \"pub_date_ordinal\", y = \"id\",  data = self.df, ci = None, marker = \".\", scatter_kws= {\"alpha\": 0.5}, line_kws = {\"color\": \"gray\"})\n",
    "\n",
    "            ax.set_ylim(self.df.id.min(), self.df.id.max())\n",
    "            ax.set_xlabel(\"Review Publication Date\")\n",
    "            ax.set_ylabel(\"Review ID\")\n",
    "            new_xlabels = [date.fromordinal(int(x_val)) for x_val in ax.get_xticks() ]\n",
    "            ax.set_xticklabels(new_xlabels)\n",
    "            plt.xticks(rotation = 90)\n",
    "\n",
    "            plt.title(\"Review URL IDs by Date\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def print_pearson(self):\n",
    "\n",
    "        r, p = scipy.stats.pearsonr(self.df.id, self.df.pub_date_ordinal)\n",
    "\n",
    "        print(\"r = {:.2}, p = {:.3}, two tailed\".format(r, p))\n",
    "\n",
    "class Decision_Tree():\n",
    "\n",
    "    def __init__(self, df, min_year):\n",
    "\n",
    "        self.df = review_df.copy()\n",
    "        self.split = 0.8 #NO REASON NOT TO GO WITH THE RULE OF THUMB\n",
    "        self.min_year = min_year\n",
    "        \n",
    "        self.df[\"is_in_period\"] = np.where(self.df[\"pub_year\"] >= self.min_year, 1, 0)\n",
    "                \n",
    "        self.df.drop(columns = [\"pub_date_ordinal\", \"pub_year\"], inplace = True)\n",
    "        \n",
    "        self.split_data() #Do this in the init so that you use the same split for each potential depth.\n",
    "\n",
    "    def split_data(self):\n",
    "\n",
    "        num_observations_total = len(self.df)\n",
    "        num_observations_train = int(num_observations_total* self.split)\n",
    "        num_observations_test = num_observations_total - num_observations_train\n",
    "        \n",
    "        self.df = self.df.iloc[np.random.permutation(self.df.index)].reset_index(drop=True)\n",
    "\n",
    "        self.train_df = self.df.head(num_observations_train).reset_index(drop = True)\n",
    "        self.test_df = self.df.tail(num_observations_test).reset_index(drop = True)\n",
    "\n",
    "        self.x_train, self.y_train = self.train_df.id, self.train_df.is_in_period\n",
    "        self.x_test, self.y_test = self.test_df.id, self.test_df.is_in_period\n",
    "\n",
    "        self.x_train = self.x_train.values.reshape(-1, 1)\n",
    "        self.y_train = self.y_train.values.reshape(-1, 1)\n",
    "        self.x_test = self.x_test.values.reshape(-1, 1)\n",
    "        self.y_test = self.y_test.values.reshape(-1, 1)\n",
    "\n",
    "    def fit_tree(self, depth = None):\n",
    "\n",
    "        self.clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth = depth)\n",
    "        self.clf = self.clf.fit(self.x_train, self.y_train)\n",
    "\n",
    "    def print_tree_accuracy(self):\n",
    "\n",
    "        pred_train = self.clf.predict(self.x_train)\n",
    "        pred_test = self.clf.predict(self.x_test)\n",
    "\n",
    "        print(\"Training Data:\")\n",
    "        print(classification_report(self.y_train, pred_train))\n",
    "\n",
    "        print(\"Test Data:\")\n",
    "        print(classification_report(self.y_test, pred_test))\n",
    "\n",
    "    def show_tree(self):\n",
    "        \n",
    "        plt.close(\"all\") #BECAUSE JUPYTER NOTEBOOKS SUCK\n",
    "\n",
    "        plt.figure(figsize = (10,10))\n",
    "        sklearn.tree.plot_tree(self.clf, feature_names= [\"id\"])\n",
    "        plt.show()\n",
    "        \n",
    "    def test_tree_depths(self, max_depth_list):\n",
    "        \n",
    "        plt.close(\"all\") #BECAUSE JUPYTER NOTEBOOKS SUCK\n",
    "        \n",
    "        for depth in max_depth_list:\n",
    "            self.fit_tree(depth)\n",
    "        \n",
    "            print(\"Max Depth: {}\".format(depth))\n",
    "            self.print_tree_accuracy()\n",
    "            self.show_tree()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, I want to visualize the relationship between review ID and review publication date. Based on previous analysis, I expect this to be roughly linear, with the exception of some short periods in which different systems appeared to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_full_data = Regression_Visualizer(review_df)\n",
    "regression_full_data.print_pearson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That image is misleading. Because there are so many points, many populated areas of the graph appear solid, masking variations in their relative densities. We can see from the pearson values that the relationship is almost perfectly linear. Based on the image alone, I would consider the possibility that an polynomial curve would more closely capture the relationship, which would sense if the rate at which users leave reviews accelerates over time. However, the linear description is close enough that I don't want to spend time exploring. \n",
    "\n",
    "Because a lot of the data comes from ancient history and apparently from the future (??), I would like to narrow in on the period that actually interests me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df_trunc = review_df.copy()\n",
    "review_df_trunc = review_df_trunc[review_df.pub_year >= 2017]\n",
    "review_df_trunc = review_df_trunc[review_df.pub_year <= 2020]\n",
    "\n",
    "regression_trunc_data = Regression_Visualizer(review_df_trunc)\n",
    "regression_trunc_data.print_pearson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The story is quite similar: the relationship is almost perfectly linear. I can also see that the model's inaccuracy is skewed in one direction (the residuals are not random! that never happens in stats class!): many review ids have publication dates later than would be predicted by the regression, but very few have publication dates earlier than would be predicted by the regression. Because I'm actually looking for a cutoff point rather than a prediction, that means that much of the error is irrelevant to the problem I'm trying to solve. In other words, the prediction is more valuable than the r-value would suggest. Regression, because it cares about errors in both directions and because it cares about magnitude of error, is actually not the perfect tool for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I reduce all dates to a binary in-period-of-interest or out-of-period-of-interest classification, the problem is better suited to creating a split. A couple of thoughts and caveats: \n",
    "\n",
    "First, in simplifying the decision-criteria, I'm taking a step away from the problem that I am ultimately trying to solve because the binary view of the time periods could potentially mask a skewed selection of dates. For example, this approach would correctly identify a range of reviews written on July 4th of 2018 as all falling within the 2018-2020 time period, but a sample comprised entirely of those reviews won't speak to change over time. I'll need to keep an eye on the purity of my splits in order to evaluate how much relevant data the suggested splits would exclude. \n",
    "\n",
    "Second, even though it would be much easier (and less memory-intensive) to input a single range of numbers into the scraper, I will also consider the possibilitiy of multiple splits. The initial analysis from over the summer suggested particular periods in which the general relationship between the IDs and review dates broke down, as if the site one day decided to reuse old identifiers for new reviews, so I would not be surprised to discover that the best fit is several non contigous ranges. I plan to test small numbers of splits and be biased towards the simplest model in which the accuracy is not prohibitively low. I suspect that with a single feature, and a small number of splits allowed, I feel like a single setting should be sufficient to control complexity. I think it's max depth. \n",
    "\n",
    "I also need to consider what success looks like by comparing the cost of false positives (out of period, in scraping range) versus false negatives (in period, out of scraping range) on the analysis. False positives cause us to lose time scraping data that we will then need to discard. False negatives are more complicated because their impact depends on whether or not they occur at random. If the data that gets excluded from our scraping range is random, it would have no cost. However, because the data is pretty linear, I feel comfortable assuming that the false negatives would be located at the start of the historical time period. To make it concrete, let's say that data from January of 2018 is underrepresented - what impact does that have? The false negatives should remain random with respect to the books covered by the missing reviews, which is good. However, a severe underrepresentation of January might depress any predictive power associated with data from that month. I'm going to use accuracy to start, but I want to keep an eye on the values associated with purity of the out-of-range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_list = range(1, 4) #I'M NOT EVEN GOING TO TEST HIGHER NUMBERS UNLESS THESE COME BACK WITH LOW ACCURACY\n",
    "year_list = [2017, 2018]\n",
    "\n",
    "for year in year_list:\n",
    "    print(\"Modeling {} cutoff\".format(year))\n",
    "    splitting_tree = Decision_Tree(review_df, year)\n",
    "    splitting_tree.test_tree_depths(depth_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These have very high accuracy across the board, which seems too good to be true. However, given the strenght of the linear correlation, it might not mean the result is invalid. For both 2017 and 2018 cutoffs, the single depth (ie, max_depth = 1) has sufficient performance to justify ignoring the more complex models. The 2018 model looks good in that it has 0.99 accuracy on test data and all the other metrics are great too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_list = range(1, 5)\n",
    "\n",
    "test_tree_2017 = Decision_Tree(review_df, 2018)\n",
    "test_tree_2017.test_tree_depths(depth_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing that I want to understand is whether this model leads us to a smaller range of potentially relevant IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = 3607950182 #id assigned to a review I wrote this week\n",
    "min_id_prev_estimate = 1484362322\n",
    "min_id_2017, min_id_2018 = 1859200000, 2235559808\n",
    "\n",
    "population_count_prev_estimate = max_id - min_id_prev_estimate\n",
    "population_count_2017 = max_id - min_id_2017\n",
    "population_count_2018 = max_id - min_id_2018\n",
    "\n",
    "print(\"\"\"\n",
    "Prev Estimate:  {:.1E} points\n",
    "2017 - 2020: {:.1E} points, {:.2%} reduction in scope\n",
    "2018 - 2020: {:.1E} points, {:.2%} reduction in scope\n",
    "\"\"\". format(population_count_prev_estimate, population_count_2017, (population_count_2017 - population_count_prev_estimate)/population_count_prev_estimate, population_count_2018, (population_count_2018 - population_count_prev_estimate)/population_count_prev_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps: The risk of a mistake with these estimations would be that we would introduce a data hole into our scraped data, most likely at the start of 2018. We can protect against this by periodically examining the distribution of dates for reviews scraped based on these new estimations. A gap in dates, especially at the start of 2018, would indicate that we should revisit this analysis. We're scraping based on these estimations starting on 10/24/2020, so any audits should be limited to data scraped after that date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate percent of reviews within the target range scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_totals_ids_in_range_est = population_count_2018\n",
    "\n",
    "review_df_in_range = review_df.copy()\n",
    "review_df_in_range = review_df_in_range[review_df.pub_year >= 2018]\n",
    "review_df_in_range = review_df_in_range[review_df.pub_year <= 2020]\n",
    "\n",
    "num_ids_in_range_scraped = len(review_df_in_range)\n",
    "perc_of_population = 100* num_ids_in_range_scraped / num_totals_ids_in_range_est\n",
    "\n",
    "print(perc_of_population)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
