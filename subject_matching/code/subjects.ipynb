{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling subject/genre information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code used to pull subject information and merge it into the Goodreads data.\n",
    "\n",
    "The main part of this code will later be included in an adjacent python script that doesn't include all the testing and fluff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input file name (no .csv) containing book meta data:\n",
      "book_data_full_isbn\n",
      "Please select the minimum number of occurences of a subject to be included as a feature\n",
      "(subject features with occurences below the specified value will be dropped):\n",
      "3\n",
      "Please input file name (no .csv) to output final dataframe to:\n",
      "final\n"
     ]
    }
   ],
   "source": [
    "## Initial arguments at the top\n",
    "input_file = input(\"Please input file name (no .csv) containing book meta data:\\n\")\n",
    "min_k = input(\"Please select the minimum number of occurences of a subject to be included as a feature\\n\"\n",
    "            + \"(subject features with occurences below the specified value will be dropped):\\n\")\n",
    "output_file = input(\"Please input file name (no .csv) to output final dataframe to:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodreads Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Goodreads data\n",
    "\n",
    "#filename = 'book_data_full_isbn.csv'\n",
    "directory = '../data/'\n",
    "gr = pd.read_csv(directory + input_file + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some isbn13 numbers are recorded as ASIN (Amazon Standard Identification Number) values. For now, these are being removed in the cleaning step, but it may be a good idea to figure out how to work around this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning ISBNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ISBN numbers have 'X' as the 13th digit. Apparently this is supposed to belong to only ISBN 10 numbers. It's possible that Open Library has somehow 'constructed' their own ISBN 13 identifiers from existing ISBN 10 numbers when the ISBN 13 identifiers were missing.\n",
    "Whether or not these should be included will depend on what ISBN values are in the Good Reads data. Currently, any ISBN numbers with alpha digits have been removed from the Goodreads data however. So far, none of them are 13 digit ISBNs that end in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ISBN Numbers\n",
    "\n",
    "## Change non-valid isbn numbers to \"None\"\n",
    "letters = re.compile(\"[A-Za-z]\")\n",
    "e_12 = re.compile(\"E\\+12\")\n",
    "\n",
    "for i in range(len(gr)):\n",
    "    if letters.search(gr.isbn13[i]) != None:\n",
    "        if e_12.search(gr.isbn13[i]) == None:\n",
    "            gr.loc[i, 'isbn13'] = 'None'\n",
    "\n",
    "            \n",
    "## Remove rows with missing (and non-valid) isbn numbers\n",
    "gr = gr[gr.isbn13 != 'None']\n",
    "gr.index = range(len(gr))\n",
    "\n",
    "\n",
    "## Expand ISBN numbers from E+12 format\n",
    "for i in range(len(gr)):\n",
    "    gr.loc[i, 'isbn13'] = str(int(pd.to_numeric(gr.loc[i, 'isbn13'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Goodreads IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goodreads IDs scraped from goodreads do not match up with the IDs stored in OL. This code will stay here for now as reference in case something changes and we do want to use this identifier. But otherwise it will currently be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Goodreads IDs\n",
    "#\n",
    "### Pull out Goodreads ids from 'editions_url' column\n",
    "#gr_id_patt = re.compile(\"(?<=/work/editions/)[0-9]+\")\n",
    "#\n",
    "#id_vec = []\n",
    "#\n",
    "#for ed in gr.editions_url:\n",
    "#    gr_id = gr_id_patt.search(ed)\n",
    "#    if gr_id != None:\n",
    "#        id_vec.append(gr_id[0])\n",
    "#    else:\n",
    "#        id_vec.append(None)\n",
    "#        \n",
    "#gr[\"gr_id\"] = id_vec\n",
    "#\n",
    "### Drop missing gr_id rows (for now)\n",
    "#gr.drop(gr.loc[gr[\"gr_id\"].isnull()].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Library python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can take individual ISBN values (or other identifiers) and return the associated json file.\n",
    "\n",
    "Issues:\n",
    "- It is likely slower than using the bulk file since it will have to re-query for each line.\n",
    "- Any slowness is going to scale up as data increases\n",
    "\n",
    "Good stuff:\n",
    "- Some books have multiple ISBNs that are close in values. The bulk json files make it difficult to use these slighlty different ISBN numbers to pull data. The OL package immediately recognizes the similar ISBN values and pulls data accordingly.\n",
    "- It can pull based on other identifiers *** To test: can we used the Amazon identifiers to get a few more books in there? If so, the above cleaning method will need to be updated to seperate out observations with Amazon IDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/internetarchive/openlibrary-client/blob/master/olclient/openlibrary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olclient.openlibrary import OpenLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Example from OL documentation\n",
    "ol = OpenLibrary()\n",
    "work = ol.Work.get(u'OL12938932W')\n",
    "editions = work.editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example format for pulling book info for particular ISBN\n",
    "isbn_test = ol.Edition.get(isbn = '9781477823835')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull data from Open Library given Goodreads ISBNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ISBN numbers from Goodreads dataset\n",
    "isbn_list = gr.isbn13.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ol_pull(ibsn_vec, keys = [\"genres\", \"subjects\"]):\n",
    "    '''\n",
    "    Take list of isbn values and return information from Open Library based on categories given by 'keys.'\n",
    "    Outputs dataframe with isbn13 and values for each listed key.\n",
    "    Books without data listed in 'keys' are not included in the output.\n",
    "    '''\n",
    "    ol_data = []\n",
    "\n",
    "    for isbn in isbn_vec:\n",
    "        book = ol.Edition.get(isbn = isbn)\n",
    "        if book != None:\n",
    "            book_dat = [book.json().get(key) for key in keys]\n",
    "            ol_data.append([isbn] + book_dat)\n",
    "\n",
    "    return(pd.DataFrame(ol_data, columns = [\"isbn13\"] + keyvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving OpenLibrary response: {'target': <function OpenLibrary._get_ol_response at 0x0000019F2E4488B0>, 'args': (<olclient.openlibrary.OpenLibrary object at 0x0000019F2E6BFF10>, '/api/books.json?bibkeys=ISBN:9780590431972'), 'kwargs': {}, 'tries': 5, 'elapsed': 7.480785}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\preston\\anaconda3\\lib\\site-packages\\backoff\\_sync.py\", line 94, in retry\n",
      "    ret = target(*args, **kwargs)\n",
      "  File \"C:\\Users\\preston\\anaconda3\\lib\\site-packages\\olclient\\openlibrary.py\", line 139, in _get_ol_response\n",
      "    response.raise_for_status()\n",
      "  File \"C:\\Users\\preston\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 941, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://openlibrary.org/api/books.json?bibkeys=ISBN:9780590431972\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://openlibrary.org/api/books.json?bibkeys=ISBN:9780590431972",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3481728c5ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0misbn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0misbn_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEdition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misbn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misbn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mbook_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyvec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\olclient\\openlibrary.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(cls, olid, isbn, oclc, lccn, ocaid)\u001b[0m\n\u001b[0;32m    521\u001b[0m                     \u001b[0mbibkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'ISBN'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0misbn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'OCLC'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moclc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'OCAID'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mocaid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LCCN'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlccn\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                     \u001b[0mbibkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbibkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                     \u001b[0molid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_olid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbibkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0molid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                         \u001b[1;31m# No edition found by bibkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\olclient\\openlibrary.py\u001b[0m in \u001b[0;36mget_olid\u001b[1;34m(cls, key, value)\u001b[0m\n\u001b[0;32m    565\u001b[0m                     \u001b[0molid\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m                 \"\"\"\n\u001b[1;32m--> 567\u001b[1;33m                 \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m                     \u001b[0mbook_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'info_url'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\olclient\\openlibrary.py\u001b[0m in \u001b[0;36mget_metadata\u001b[1;34m(cls, key, value)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/api/books.json?bibkeys=%s:%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ol_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\backoff\\_sync.py\u001b[0m in \u001b[0;36mretry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mexception\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mmax_tries_exceeded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtries\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_tries_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\olclient\\openlibrary.py\u001b[0m in \u001b[0;36m_get_ol_response\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;34m\"\"\"Makes best effort to perform request w/ exponential backoff\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://openlibrary.org/api/books.json?bibkeys=ISBN:9780590431972"
     ]
    }
   ],
   "source": [
    "## Dictionary keys to pull\n",
    "#keyvec = [\"isbn_13\", \"title\", \"genres\", \"subjects\", \"description\"]\n",
    "keyvec = [\"genres\", \"subjects\"]\n",
    "\n",
    "## For each ISBN, pull book data according to 'keyvec' and append to list\n",
    "ol_data = []\n",
    "\n",
    "for isbn in isbn_list:\n",
    "    book = ol.Edition.get(isbn = isbn)\n",
    "    if book != None:\n",
    "        book_dat = [book.json().get(key) for key in keyvec]\n",
    "        ol_data.append([isbn] + book_dat)\n",
    "\n",
    "## Create dataframe to work with\n",
    "isbn_df = pd.DataFrame(ol_data, columns = [\"isbn13\"] + keyvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gr_subjects.csv\n",
      "subject_feats.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_df = pd.read_csv(\"../output/gr_subjects.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert subject data to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_feat_sel(df, k = 1):\n",
    "    '''\n",
    "    Drop subject features for which there exist k or less books having that subject\n",
    "    '''\n",
    "    sub_counts = df.sum(axis = 0)\n",
    "    to_drop = sub_counts[sub_counts < k].index\n",
    "    \n",
    "    return(df.drop(columns = to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert lists of subjects to singular clean string\n",
    "\n",
    "## Pattern to remove any non-alphabetic characters\n",
    "letter_patt = re.compile(\"[^A-Za-z \\t]\")\n",
    "\n",
    "## Subject lists\n",
    "sub_lists = isbn_df.subjects[isbn_df.subjects.notna()]\n",
    "## Index for books with subjects\n",
    "sub_index = sub_lists.index\n",
    "\n",
    "## For each book, combine subject lists into one string and remove punctuation/digits\n",
    "sub_text = [\"\".join(l).lower() for l in sub_lists]\n",
    "sub_text = [re.sub(letter_patt, \" \", t) for t in sub_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem subjects for each book\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "\n",
    "stemmed_subs = []\n",
    "\n",
    "for phrase in sub_text:\n",
    "    stems = \" \".join(set([lancaster.stem(word) for word in str.split(phrase)]))\n",
    "    stemmed_subs.append(stems)\n",
    "\n",
    "sub_clean = pd.Series(stemmed_subs, name = \"clean_subjects\", index = sub_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn each possible subject word into a seperate feature and merge back to previous ISBNs\n",
    "\n",
    "## Binary counts for each subject word\n",
    "cv = CountVectorizer(stop_words = \"english\", binary = True)\n",
    "cv.fit(sub_clean)\n",
    "sub_feats = cv.transform(sub_clean)\n",
    "\n",
    "## Change dense data to normal for sake of merging\n",
    "col_names = np.sort(list(cv.vocabulary_.keys()))\n",
    "\n",
    "nondense = pd.DataFrame(sub_feats.todense(), columns = col_names, index = sub_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign cleaned subjects to original ISBNs\n",
    "isbn_clean = isbn_df.join(sub_clean, how = \"left\")[[\"isbn13\", \"clean_subjects\"]]\n",
    "\n",
    "## Assign features to original ISBNs\n",
    "isbn_feats = isbn_clean.join(nondense, how = \"left\")\n",
    "\n",
    "## Remove features with little coverage\n",
    "isbn_feats_clean = naive_feat_sel(isbn_feats, k = min_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following statistics are based on the number of valid ISBNs, not the number of observations in the original dataset. Invalid ISBNs, Amazon IDs, etc. have been excluded from the matching and are subsequently not included here. This upweights these statistics slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched ISBN count: 102\n",
      "Unmatched ISBN proportion: 5.16 %\n"
     ]
    }
   ],
   "source": [
    "## Direct match rate\n",
    "nomatch = len(isbn_list) - len(isbn_df)\n",
    "total = len(isbn_list)\n",
    "\n",
    "print(\"Unmatched ISBN count:\", nomatch)\n",
    "\n",
    "print(\"Unmatched ISBN proportion:\", round(nomatch / total * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subject/genre rate\n",
    "val_vec = []\n",
    "\n",
    "for i in range(len(isbn_df)):\n",
    "    val = 0\n",
    "    if isbn_df.genres[i] != None:\n",
    "        val += 1\n",
    "    if isbn_df.subjects[i] != None:\n",
    "        val += 2\n",
    "#    if isbn_df.description[i] != None:\n",
    "#        val += 4\n",
    "\n",
    "    val_vec.append(val)\n",
    "    \n",
    "val_vec = np.array(val_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: len(val_vec) and total do not match in terms of length!\n",
    "## That might have to do with unmatched ibsns--double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ISBNs with no subject or genre info: 887 ( 44.84 %)\n"
     ]
    }
   ],
   "source": [
    "noinfo = ((val_vec == 0)).sum()\n",
    "print(\"Number of ISBNs with no subject or genre info:\", noinfo, \"(\", round((noinfo / total) * 100, 2), \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ISBNs with only genre info: 0 ( 0.0 %)\n"
     ]
    }
   ],
   "source": [
    "genre_info = ((val_vec == 1)).sum()\n",
    "print(\"Number of ISBNs with only genre info:\", genre_info, \"(\", round((genre_info / total) * 100, 2), \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ISBNs with only subject info: 774 ( 39.13 %)\n"
     ]
    }
   ],
   "source": [
    "sub_info = ((val_vec == 2)).sum()\n",
    "print(\"Number of ISBNs with only subject info:\", sub_info, \"(\", round((sub_info / total) * 100, 2), \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ISBNs with genre and subject info: 215 ( 10.87 %)\n"
     ]
    }
   ],
   "source": [
    "both_info = ((val_vec == 3)).sum()\n",
    "print(\"Number of ISBNs with genre and subject info:\", both_info, \"(\", round((both_info / total) * 100, 2), \"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 348)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isbn_feats_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_feats_clean.to_csv(\"../output/\" + output_file + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also, output a dataframe for every possible subject to decode stemmed features\n",
    "uniq_subs = set(str.split(\" \".join(sub_text)))\n",
    "uniq_stems = [lancaster.stem(word) for word in uniq_subs]\n",
    "\n",
    "decoder = pd.DataFrame({\"stem\":uniq_stems, \"word\":list(uniq_subs)})\n",
    "decoder.sort_values(by = [\"stem\", \"word\"], inplace = True)\n",
    "\n",
    "decoder.to_csv(\"../output/feature_decoder.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Library Bulk Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic look at data formatting\n",
    "\n",
    "Data can be found here:\n",
    "\n",
    "https://openlibrary.org/developers/dumps\n",
    "\n",
    "On account of the dump files being gigantic, they are not currently stored in the Git repository. The following code assumes the 'editions' bulk file has been downloaded and stored in the 'data' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISBN Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take ISBN numbers from Goodreads dataset\n",
    "isbn_list = gr.isbn13.to_list()\n",
    "isbn_patt = re.compile(\"(?<=isbn_13\\': \\[\\').{13}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull appropriate key values from json file\n",
    "keys = [\"isbn_13\", \"title\", \"genre\", \"subjects\"]\n",
    "ol_vec = []\n",
    "\n",
    "## For each json line, if it contains an ISBN value and that value\n",
    " # is in the Goodreads ISBNs, add it to a list\n",
    "with open('../data/edition_json.txt') as json_file:\n",
    "    for line in json_file:\n",
    "        if (re.search('\\\"isbn_13\\\"', line) != None): \n",
    "            if isbn_patt.search(line) != None:\n",
    "                isbn_num = isbn_patt.search(line)[0]\n",
    "                if str(isbn_num) in isbn_list:\n",
    "                    l = [json.loads(line).get(key) for key in keys]\n",
    "                    ol_vec.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Open Library list to dataframe with properly formatted ISBN numbers\n",
    "isbn_df = pd.DataFrame(ol_vec, columns = keys)\n",
    "\n",
    "clean_isbn = []\n",
    "\n",
    "for l in isbn_df.isbn_13:\n",
    "    clean_isbn.append(l[0])\n",
    "\n",
    "isbn_df[\"isbn13\"] = clean_isbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a simple inner join to make sure things are lined up correctly\n",
    "merge_check = gr.merge(isbn_df, on = 'isbn13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final merge into original Goodreads dataset\n",
    "gr_merge = gr.merge(isbn_df, how = 'left', on = 'isbn13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this method is not currently being used because it is easier to work with the Open Library package rather than the raw data. This is kept here in case the raw files are needed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodreads ID Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this has been shown to not be useful. But it is commented out in case it becomes useful otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take ISBN numbers from Goodreads dataset\n",
    "#gr_id_list = gr.gr_id.to_list()\n",
    "#gr_patt = re.compile(\"(?<=goodreads\\\": \\[\\\")[0-9]+\")\n",
    "#\n",
    "### Pull appropriate key values from json file\n",
    "#keys = [\"identifiers\", \"isbn_13\", \"title\", \"genres\", \"subjects\"]\n",
    "#ol_vec = []\n",
    "#\n",
    "#with open('../data/edition_json.txt') as json_file:\n",
    "#    for line in json_file:\n",
    "#        if (re.search('\\\"goodreads\\\"', line) != None): \n",
    "#            if gr_patt.search(line) != None:\n",
    "#                gr_num = gr_patt.search(line)[0]\n",
    "#                if gr_num in gr_id_list:\n",
    "#                    l = [json.loads(line).get(key) for key in keys]\n",
    "#                    ol_vec.append(l)\n",
    "##\n",
    "### Convert open library list to dataframe with properly formatted ISBN numbers\n",
    "#ol_df = pd.DataFrame(ol_vec, columns = keys)\n",
    "#\n",
    "#gr_vec = []\n",
    "#\n",
    "#for i in range(len(ol_df)):\n",
    "#    gr_vec.append(ol_df.identifiers[i][\"goodreads\"][0])\n",
    "#    \n",
    "#ol_df[\"gr_id\"] = gr_vec\n",
    "##ol_df.drop(columns = \"identifiers\", inplace = True)\n",
    "#\n",
    "#merge_check = gr.merge(ol_df, on = 'gr_id')\n",
    "##merge_check\n",
    "#\n",
    "#gr_merge = gr.merge(ol_df, how = 'left', on = 'gr_id')\n",
    "##gr_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch/checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really just a spot to test out stuff and easily look at the content of the jsons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pull json lines\n",
    "keys = [\"isbn_13\", \"title\", \"subjects\"]\n",
    "dat = []\n",
    "\n",
    "with open('../data/test.txt') as json_file:\n",
    "    for line in json_file:\n",
    "        dat.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull appropriate key values from json file\n",
    "keys = [\"identifiers\", \"isbn_13\", \"title\", \"subjects\"]\n",
    "ol = []\n",
    "\n",
    "with open('../data/test.txt') as json_file:\n",
    "    for line in json_file:\n",
    "        if (re.search('\\\"goodreads\\\"', line) != None): \n",
    "            if gr_patt.search(line) != None:\n",
    "                l = [json.loads(line).get(key) for key in keys]\n",
    "                ol.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys = [\"isbn_13\", \"title\", \"subjects\", \"source_records\"]\n",
    "dat = []\n",
    "\n",
    "with open('../data/test.txt') as json_file:\n",
    "    for line in json_file:\n",
    "        if re.search(re.compile('subject'), line) != None:\n",
    "            l = [json.loads(line).get(key) for key in keys]\n",
    "            dat.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
